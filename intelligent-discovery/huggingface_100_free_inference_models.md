# ü§ó HuggingFace 100+ Free Inference Models

## üìä Sorted by Downloads (Most Popular First)

### üî§ **Embeddings & Sentence Similarity**
| Model | Downloads | Task | Description |
|-------|-----------|------|-------------|
| `sentence-transformers/all-MiniLM-L6-v2` | 86M | Embeddings | Best general-purpose embeddings |
| `sentence-transformers/all-mpnet-base-v2` | 18M | Embeddings | High-quality sentence embeddings |
| `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` | 12M | Multilingual | Multilingual paraphrase detection |
| `BAAI/bge-large-en-v1.5` | 5.6M | Embeddings | State-of-the-art English embeddings |
| `BAAI/bge-m3` | 4.5M | Multilingual | Multilingual dense retrieval |
| `jinaai/jina-embeddings-v3` | 4.4M | Embeddings | Latest Jina embeddings |
| `sentence-transformers/paraphrase-MiniLM-L6-v2` | 4.3M | Embeddings | Compact paraphrase embeddings |
| `intfloat/multilingual-e5-large` | 3.7M | Multilingual | Multilingual E5 embeddings |
| `BAAI/bge-base-en-v1.5` | 3.4M | Embeddings | BGE base English model |
| `BAAI/bge-small-en-v1.5` | 3.4M | Embeddings | Compact BGE embeddings |
| `sentence-transformers/all-MiniLM-L12-v2` | 3.2M | Embeddings | Larger MiniLM variant |
| `intfloat/multilingual-e5-large-instruct` | 2.1M | Multilingual | Instruction-tuned multilingual |
| `mixedbread-ai/mxbai-embed-large-v1` | 2.0M | Embeddings | High-performance embeddings |
| `Alibaba-NLP/gte-large-en-v1.5` | 1.9M | Embeddings | GTE English embeddings |
| `sentence-transformers/all-roberta-large-v1` | 1.9M | Embeddings | RoBERTa-based embeddings |
| `sentence-transformers/multi-qa-MiniLM-L6-cos-v1` | 1.8M | QA | Question-answering embeddings |
| `intfloat/multilingual-e5-small` | 1.6M | Multilingual | Compact multilingual E5 |
| `nomic-ai/nomic-embed-text-v1.5` | 1.6M | Embeddings | Nomic embeddings v1.5 |
| `intfloat/multilingual-e5-base` | 1.5M | Multilingual | Base multilingual E5 |
| `sentence-transformers/distiluse-base-multilingual-cased-v2` | 1.3M | Multilingual | Multilingual DistilUSE |

### üé≠ **Fill-Mask Models**
| Model | Downloads | Task | Description |
|-------|-----------|------|-------------|
| `google-bert/bert-base-uncased` | 55M | Fill-mask | Original BERT base uncased |
| `FacebookAI/roberta-large` | 12M | Fill-mask | Large RoBERTa model |
| `FacebookAI/xlm-roberta-base` | 11M | Multilingual | Multilingual RoBERTa base |
| `distilbert/distilbert-base-uncased` | 10M | Fill-mask | Distilled BERT uncased |
| `FacebookAI/roberta-base` | 8.8M | Fill-mask | RoBERTa base model |
| `google-bert/bert-base-multilingual-cased` | 6.7M | Multilingual | Multilingual BERT cased |
| `emilyalsentzer/Bio_ClinicalBERT` | 4.9M | Medical | Clinical domain BERT |
| `microsoft/deberta-v3-large` | 4.6M | Fill-mask | DeBERTa v3 large |
| `FacebookAI/xlm-roberta-large` | 4.1M | Multilingual | Large multilingual RoBERTa |
| `google-bert/bert-base-cased` | 3.9M | Fill-mask | BERT base cased |
| `google-bert/bert-base-chinese` | 3.5M | Chinese | Chinese BERT |
| `nlpaueb/legal-bert-base-uncased` | 3.2M | Legal | Legal domain BERT |
| `microsoft/mdeberta-v3-base` | 4.2M | Fill-mask | Multilingual DeBERTa v3 |

### ü§ñ **Text Generation Models**
| Model | Downloads | Task | Description |
|-------|-----------|------|-------------|
| `openai-community/gpt2` | 14M | Text generation | Original GPT-2 |
| `Qwen/Qwen2.5-14B-Instruct` | 12M | Instruction | Qwen 2.5 14B instruct |
| `meta-llama/Llama-3.1-8B-Instruct` | 10M | Instruction | Llama 3.1 8B instruct |
| `Qwen/Qwen2.5-7B-Instruct` | 7.9M | Instruction | Qwen 2.5 7B instruct |
| `Qwen/Qwen3-4B-Base` | 7.5M | Base | Qwen3 4B base model |
| `facebook/opt-125m` | 5.3M | Text generation | OPT 125M parameters |
| `Qwen/Qwen3-8B-Base` | 4.2M | Base | Qwen3 8B base |
| `Qwen/Qwen3-0.6B` | 4.1M | Base | Compact Qwen3 0.6B |
| `distilbert/distilgpt2` | 3.5M | Text generation | Distilled GPT-2 |
| `meta-llama/Llama-3.2-1B-Instruct` | 3.4M | Instruction | Llama 3.2 1B instruct |
| `google/gemma-3-1b-it` | 3.1M | Instruction | Gemma 3 1B instruct |
| `meta-llama/Llama-3.2-1B` | 2.9M | Base | Llama 3.2 1B base |
| `Qwen/Qwen3-8B` | 2.7M | Base | Qwen3 8B model |

### üìù **Text Classification**
| Model | Downloads | Task | Description |
|-------|-----------|------|-------------|
| `distilbert/distilbert-base-uncased-finetuned-sst-2-english` | 3.1M | Sentiment | SST-2 sentiment analysis |
| `cardiffnlp/twitter-roberta-base-sentiment-latest` | 3.1M | Sentiment | Twitter sentiment analysis |
| `BAAI/bge-reranker-v2-m3` | 2.6M | Reranking | BGE reranker v2 |
| `facebook/roberta-hate-speech-dynabench-r4-target` | 1.8M | Hate speech | Hate speech detection |
| `nlptown/bert-base-multilingual-uncased-sentiment` | 1.7M | Sentiment | Multilingual sentiment |
| `microsoft/deberta-large-mnli` | 1.6M | NLI | Natural language inference |
| `BAAI/bge-reranker-base` | 1.5M | Reranking | BGE reranker base |
| `yiyanghkust/finbert-tone` | 1.5M | Finance | Financial sentiment |
| `ProsusAI/finbert` | 1.3M | Finance | Financial text analysis |
| `microsoft/deberta-xlarge-mnli` | 786K | NLI | XLarge DeBERTa MNLI |

### üè∑Ô∏è **Token Classification (NER/POS)**
| Model | Downloads | Task | Description |
|-------|-----------|------|-------------|
| `dslim/bert-base-NER` | 2.2M | NER | BERT-based NER |
| `w11wo/indonesian-roberta-base-posp-tagger` | 1.6M | POS | Indonesian POS tagging |
| `dbmdz/bert-large-cased-finetuned-conll03-english` | 1.3M | NER | CoNLL-03 English NER |
| `gilf/french-camembert-postag-model` | 1.2M | POS | French POS tagging |
| `flair/ner-english-fast` | 950K | NER | Fast English NER |
| `oliverguhr/fullstop-punctuation-multilang-large` | 917K | Punctuation | Multilingual punctuation |
| `tsmatz/xlm-roberta-ner-japanese` | 900K | NER | Japanese NER |
| `kredor/punctuate-all` | 748K | Punctuation | Universal punctuation |
| `cahya/NusaBert-ner-v1.3` | 724K | NER | Indonesian NER |
| `EmergentMethods/gliner_medium_news-v2.1` | 554K | NER | GLiNER news domain |
| `obi/deid_roberta_i2b2` | 485K | De-identification | Medical text de-identification |
| `MMG/xlm-roberta-large-ner-spanish` | 479K | NER | Spanish NER |
| `tner/roberta-large-tweetner7-all` | 461K | NER | Twitter NER |

### ‚ùì **Question Answering**
| Model | Downloads | Task | Description |
|-------|-----------|------|-------------|
| `deepset/roberta-base-squad2` | 2.6M | QA | RoBERTa SQuAD 2.0 |
| `dmis-lab/biobert-large-cased-v1.1-squad` | 563K | Medical QA | Biomedical QA |
| `google-bert/bert-large-uncased-whole-word-masking-finetuned-squad` | 283K | QA | BERT large SQuAD |
| `distilbert/distilbert-base-cased-distilled-squad` | 246K | QA | DistilBERT SQuAD |
| `monologg/koelectra-small-v2-distilled-korquad-384` | 243K | QA | Korean QA |
| `deepset/bert-large-uncased-whole-word-masking-squad2` | 206K | QA | BERT large SQuAD 2.0 |
| `timpal0l/mdeberta-v3-base-squad2` | 133K | QA | mDeBERTa SQuAD 2.0 |
| `distilbert/distilbert-base-uncased-distilled-squad` | 106K | QA | DistilBERT SQuAD |
| `deepset/bert-base-cased-squad2` | 79K | QA | BERT base SQuAD 2.0 |
| `deepset/xlm-roberta-large-squad2` | 68K | QA | XLM-RoBERTa SQuAD 2.0 |

### üìÑ **Summarization**
| Model | Downloads | Task | Description |
|-------|-----------|------|-------------|
| `facebook/bart-large-cnn` | 3.9M | Summarization | BART CNN summarization |
| `sshleifer/distilbart-cnn-12-6` | 1.4M | Summarization | DistilBART CNN |
| `google/pegasus-xsum` | 131K | Summarization | PEGASUS XSum |
| `philschmid/bart-large-cnn-samsum` | 111K | Summarization | BART SAMSum dialogue |
| `sshleifer/distilbart-xsum-12-6` | 52K | Summarization | DistilBART XSum |
| `Falconsai/text_summarization` | 50K | Summarization | General text summarization |
| `csebuetnlp/mT5_multilingual_XLSum` | 44K | Summarization | Multilingual mT5 XLSum |
| `sshleifer/distilbart-cnn-6-6` | 33K | Summarization | Compact DistilBART |
| `knkarthick/MEETING_SUMMARY` | 32K | Summarization | Meeting summarization |

### üåç **Translation**
| Model | Downloads | Task | Description |
|-------|-----------|------|-------------|
| `google-t5/t5-small` | 3.4M | Translation | T5 small model |
| `google-t5/t5-base` | 1.7M | Translation | T5 base model |
| `google-t5/t5-3b` | 1.1M | Translation | T5 3B model |
| `google-t5/t5-11b` | 547K | Translation | T5 11B model |
| `facebook/nllb-200-distilled-600M` | 481K | Translation | NLLB 200 languages |
| `Helsinki-NLP/opus-mt-fr-en` | 786K | Translation | French to English |
| `Helsinki-NLP/opus-mt-ru-en` | 402K | Translation | Russian to English |
| `google-t5/t5-large` | 378K | Translation | T5 large model |
| `Helsinki-NLP/opus-mt-nl-en` | 329K | Translation | Dutch to English |
| `Helsinki-NLP/opus-mt-de-en` | 322K | Translation | German to English |
| `Helsinki-NLP/opus-mt-it-en` | 303K | Translation | Italian to English |
| `Helsinki-NLP/opus-mt-zh-en` | 278K | Translation | Chinese to English |
| `Helsinki-NLP/opus-mt-ar-en` | 274K | Translation | Arabic to English |
| `Helsinki-NLP/opus-mt-es-en` | 221K | Translation | Spanish to English |

## üîß **Usage Examples**

### Text Generation
```python
import requests
API_URL = "https://api-inference.huggingface.co/models/openai-community/gpt2"
headers = {"Authorization": f"Bearer {API_KEY}"}
response = requests.post(API_URL, headers=headers, json={"inputs": "Hello world"})
```

### Fill-Mask
```python
API_URL = "https://api-inference.huggingface.co/models/google-bert/bert-base-uncased"
response = requests.post(API_URL, headers=headers, json={"inputs": "Paris is the [MASK] of France."})
```

### Question Answering
```python
API_URL = "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2"
response = requests.post(API_URL, headers=headers, json={
    "inputs": {
        "question": "What is HuggingFace?",
        "context": "HuggingFace is a company that provides AI models."
    }
})
```

### Embeddings
```python
API_URL = "https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2"
response = requests.post(API_URL, headers=headers, json={"inputs": "This is a sentence."})
```

## ‚ö†Ô∏è **Important Notes**

- **Rate Limits**: ~Few hundred requests per hour for free users
- **Cold Start**: Models may return 503 status while loading (normal)
- **Best Performance**: Most popular models load faster
- **Upgrade**: PRO account ($9/month) for higher limits
- **Authentication**: Requires HuggingFace API token

## üìà **Performance Tiers**

**üöÄ Ultra Popular (10M+ downloads)**
- Fastest loading, most reliable
- Best for production prototypes

**‚ö° Popular (1M+ downloads)**  
- Good performance, reliable
- Suitable for development

**üß™ Emerging (100K+ downloads)**
- May have cold starts
- Good for experimentation

---

*Total Models Listed: 130+*  
*Last Updated: August 2025*